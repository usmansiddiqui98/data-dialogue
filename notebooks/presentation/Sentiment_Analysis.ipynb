{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from src.models.sentiment_analysis.training_pipeline import train_models, find_best_model\n",
    "\n",
    "from src.data.generate_oversample import generate_oversample\n",
    "\n",
    "from src.models.sentiment_analysis.log_reg import LogReg\n",
    "from src.models.sentiment_analysis.lstm import BasicLSTM\n",
    "from src.models.sentiment_analysis.naive_bayes import Naivebayes\n",
    "from src.models.sentiment_analysis.pre_trained.bert_fine_tuned import BertFineTuned\n",
    "from src.models.sentiment_analysis.pre_trained.siebert import Siebert\n",
    "from src.models.sentiment_analysis.svm import SVM\n",
    "from src.models.sentiment_analysis.xg_boost import XgBoost\n",
    "from src.models.sentiment_analysis.xg_boost_svd import XgBoostSvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>negative</td>\n",
       "      <td>26/2/21</td>\n",
       "      <td>This is an okay gift box, only if you like med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>negative</td>\n",
       "      <td>18/12/19</td>\n",
       "      <td>It looks llike I just walked into a raw deal. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>negative</td>\n",
       "      <td>19/1/20</td>\n",
       "      <td>Thank god that i tasted the metal before i swa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>negative</td>\n",
       "      <td>13/9/20</td>\n",
       "      <td>This product was very good when I began buying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5443</th>\n",
       "      <td>negative</td>\n",
       "      <td>10/7/20</td>\n",
       "      <td>Once again, Paragon has disappointed with this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5444 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment      Time                                               Text\n",
       "0     positive   18/6/21  This is a very healthy dog food. Good for thei...\n",
       "1     positive    7/7/21  I've been very pleased with the Natural Balanc...\n",
       "2     positive   18/6/21  Before I was educated about feline nutrition, ...\n",
       "3     positive    7/7/21  My holistic vet recommended this, along with a...\n",
       "4     positive    1/7/21  I bought this coffee because its much cheaper ...\n",
       "...        ...       ...                                                ...\n",
       "5439  negative   26/2/21  This is an okay gift box, only if you like med...\n",
       "5440  negative  18/12/19  It looks llike I just walked into a raw deal. ...\n",
       "5441  negative   19/1/20  Thank god that i tasted the metal before i swa...\n",
       "5442  negative   13/9/20  This product was very good when I began buying...\n",
       "5443  negative   10/7/20  Once again, Paragon has disappointed with this...\n",
       "\n",
       "[5444 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/raw/reviews.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PP] Preprocessing complete\n",
      "[FE] finished lowercase count...\n",
      "[FE] finished uppercase count...\n",
      "[FE] finished uppercase ratio...\n",
      "[FE] finished punc count...\n",
      "[FE] finished pos tags...\n",
      "[FE] finished pos tag count...\n",
      "[FE] finished tokenized_untokenized count...\n",
      "[FE] finished num words misspelled...\n",
      "[FE] finished polarity...\n",
      "[FE] finished subjectivity...\n",
      "[FE] finished pos neg count...\n",
      "[FE] finished adding features...\n"
     ]
    }
   ],
   "source": [
    "from src.data.make_dataset import main as make_dataset\n",
    "# Create the processed and feature engineered dataset\n",
    "data = pd.read_csv(\"../../data/raw/reviews.csv\")\n",
    "X_train, X_test, y_train, y_test = make_dataset(\n",
    "        data,\n",
    "        train_split_output_filepath=\"../../data/processed/train_final_processed_reviews.csv\",\n",
    "        test_split_output_filepath=\"../../data/processed/test_final_processed_reviews.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "make_dataset function takes in the raw data frame, and two output file paths.\n",
    "\n",
    "make_dataset makes use of our Preprocessor and FeatureEngineer classes to process the data, performs a train-test split, and returns the respective X/y train and test sets.\n",
    "The train and test sets are also written as csv files so that we can run make_dataset once and load the processed data should we need it again.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "- Resolve contractions\n",
    "- Remove numbers and special characters\n",
    "- Lower case all words and remove punctuations\n",
    "- Remove our custom list of stop words\n",
    "- Find part-of-speech(POS) tag for each token using nltk\n",
    "- Use nltk's WordNetLemmatizer to lemmatize each token based on POS tag\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Pandarallel was used to parallelize feature engineering steps.\n",
    "The dataframe is split and processed in parallel, allowing us to speed up an otherwise single-threaded operation\n",
    "\n",
    "<b>lowercase_count</b>: number of lower case words\n",
    "\n",
    "<b>uppercase_count</b>: number of upper case words\n",
    "\n",
    "<b>uppercase_ratio</b>: ratio of uppercase words to total tokens\n",
    "\n",
    "<b>punc_count</b>: number of punctuations\n",
    "\n",
    "<b>num_digits</b>: number of cardinal digits\n",
    "\n",
    "<b>num_verbs</b>: number of verbs based on POS\n",
    "\n",
    "<b>num_nouns</b>: number of nouns based on POS\n",
    "\n",
    "<b>num_tokens_cleaned</b>: number of words after preprocessing\n",
    "\n",
    "<b>num_tokens_raw</b>: number of words before preprocessing\n",
    "\n",
    "<b>num_words_misspelled</b>: number of misspelled words using spellchecker\n",
    "\n",
    "<b>polarity</b>: compound polarity score of a sentence using VaderSentiment\n",
    "\n",
    "<b>subjectivity</b>: subjectivity score of a sentence using TextBlob\n",
    "\n",
    "<b>num_pos_words</b>: number of positive words in a sentence using VaderSentiment\n",
    "\n",
    "<b>num_neg_words</b>: number of negative words in a sentence using VaderSentiment\n",
    "\n",
    "### Oversampling\n",
    "\n",
    "<b>BERT Augmentation for Text Class Imbalance</b>:\n",
    "\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers and is a language representation model.\n",
    "It uses masked word prediction by hiding keywords in sentences and letting BERT guess what they are.\n",
    "Next sentence prediction is also used, teaching BERT to recognize longer-term dependencies across sentences.\n",
    "Oversampled training data was only used for naive bayes as that was the only model with significant impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lowercase_count</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>num_digits</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_tokens_cleaned</th>\n",
       "      <th>num_tokens_raw</th>\n",
       "      <th>num_words_misspelled</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>num_pos_words</th>\n",
       "      <th>num_neg_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disappointed.  The big boxes had a very differ...</td>\n",
       "      <td>disappointed big box very different flavor tha...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.6848</td>\n",
       "      <td>0.603148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a wonderfully refreshing citrus drink....</td>\n",
       "      <td>wonderfully refresh citrus drink not too sweet...</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.685354</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just brewed my first cup and the aroma and f...</td>\n",
       "      <td>brew my first cup aroma flavor both speak autu...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.491111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It taste okay, but they are  not soft and the ...</td>\n",
       "      <td>taste okay but not soft taste not stay very lo...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.456667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These are good chips.  I like the texture and ...</td>\n",
       "      <td>good chip like texture easy chew than regular ...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>0.558974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Disappointed.  The big boxes had a very differ...   \n",
       "1  This is a wonderfully refreshing citrus drink....   \n",
       "2  I just brewed my first cup and the aroma and f...   \n",
       "3  It taste okay, but they are  not soft and the ...   \n",
       "4  These are good chips.  I like the texture and ...   \n",
       "\n",
       "                                        cleaned_text  lowercase_count  \\\n",
       "0  disappointed big box very different flavor tha...               23   \n",
       "1  wonderfully refresh citrus drink not too sweet...               87   \n",
       "2  brew my first cup aroma flavor both speak autu...               27   \n",
       "3  taste okay but not soft taste not stay very lo...               24   \n",
       "4  good chip like texture easy chew than regular ...               26   \n",
       "\n",
       "   uppercase_count  uppercase_ratio  punc_count  num_digits  num_verbs  \\\n",
       "0                0         0.000000           3           0          1   \n",
       "1                0         0.000000           7           0          7   \n",
       "2                1         0.032258           3           0          1   \n",
       "3                0         0.000000           3           0          2   \n",
       "4                0         0.000000           3           0          0   \n",
       "\n",
       "   num_nouns  num_tokens_cleaned  num_tokens_raw  num_words_misspelled  \\\n",
       "0          3                  16              24                     0   \n",
       "1         12                  45              88                     2   \n",
       "2          8                  16              31                     0   \n",
       "3          3                  13              25                     2   \n",
       "4          6                  13              27                     0   \n",
       "\n",
       "   polarity  subjectivity  num_pos_words  num_neg_words  \n",
       "0   -0.6848      0.603148              0              0  \n",
       "1    0.9502      0.685354              2              0  \n",
       "2    0.8283      0.491111              1              0  \n",
       "3    0.1154      0.456667              0              0  \n",
       "4    0.8858      0.558974              0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "- Initialise all models and store in dict\n",
    "- Train all models on same X_train, other than naive bayes which trained on an oversampled version of X_train\n",
    "- Save (pickle) each of the trained models\n",
    "- Evaluation of models\n",
    "    - Load saved models\n",
    "    - Predict on same X_test\n",
    "    - Calculate metrics\n",
    "- Select best model\n",
    "\n",
    "### Models Used\n",
    "\n",
    "- XgBoost\n",
    "- XgBoost with singular value decomposition\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Naivebayes\n",
    "- BertFineTuned (our own fine-tuned checkpoint of bert base cased)\n",
    "- Siebert (pre-trained checkpoint of RoBERTa-large)\n",
    "- BasicLSTM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf6ba7418514c3f91d11672b716a02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Loaded model: xg_boost\n",
      "Accuracy: 0.8778696051423324\n",
      "Precision: 0.9068923821039904\n",
      "Recall: 0.9305210918114144\n",
      "F1 Score: 0.9185548071034905\n",
      "ROC AUC: 0.8292181430788521\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: xg_boost_svd\n",
      "Accuracy: 0.8888888888888888\n",
      "Precision: 0.9043683589138135\n",
      "Recall: 0.9503722084367245\n",
      "F1 Score: 0.9267997580157289\n",
      "ROC AUC: 0.8320765635823199\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: log_reg\n",
      "Accuracy: 0.8659320477502296\n",
      "Precision: 0.8909952606635071\n",
      "Recall: 0.9330024813895782\n",
      "F1 Score: 0.9115151515151515\n",
      "ROC AUC: 0.8039570710834816\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: svm\n",
      "Accuracy: 0.8732782369146006\n",
      "Precision: 0.8892773892773893\n",
      "Recall: 0.9466501240694789\n",
      "F1 Score: 0.9170673076923077\n",
      "ROC AUC: 0.8054805390665416\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: naive_bayes\n",
      "Accuracy: 0.8494031221303948\n",
      "Precision: 0.9158031088082902\n",
      "Recall: 0.8771712158808933\n",
      "F1 Score: 0.8960709759188846\n",
      "ROC AUC: 0.8237446185411533\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: bert_fine_tuned\n",
      "Accuracy: 0.9494949494949495\n",
      "Precision: 0.9723270440251572\n",
      "Recall: 0.9590570719602978\n",
      "F1 Score: 0.9656464709556527\n",
      "ROC AUC: 0.9406592780296188\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: siebert\n",
      "Siebert predicting on GPU\n",
      "Accuracy: 0.9504132231404959\n",
      "Precision: 0.9723618090452262\n",
      "Recall: 0.9602977667493796\n",
      "F1 Score: 0.9662921348314607\n",
      "ROC AUC: 0.9412796254241598\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "Loaded model: lstm\n",
      "Accuracy: 0.7465564738292011\n",
      "Precision: 0.7462825278810409\n",
      "Recall: 0.9962779156327544\n",
      "F1 Score: 0.8533475026567482\n",
      "ROC AUC: 0.5158068023393454\n",
      "________________________________________________________________________________\n",
      "Best model: siebert\n",
      "Accuracy: 0.9504132231404959\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "\n",
    "# Load the data\n",
    "train_filepath = os.path.join(BASE_DIR, \"data/processed/train_final_processed_reviews.csv\")\n",
    "train_oversample_filepath = os.path.join(BASE_DIR, \"data/processed/train_oversample_final_processed_reviews.csv\")\n",
    "test_filepath = os.path.join(BASE_DIR, \"data/processed/test_final_processed_reviews.csv\")\n",
    "\n",
    "# If preprocessed and oversampled data is saved to csv, load them\n",
    "if os.path.exists(train_filepath) and os.path.exists(test_filepath) and os.path.exists(train_oversample_filepath):\n",
    "    train = pd.read_csv(train_filepath, index_col=\"Unnamed: 0\")\n",
    "    train_os = pd.read_csv(train_oversample_filepath, index_col=\"Unnamed: 0\")\n",
    "    test = pd.read_csv(test_filepath, index_col=\"Unnamed: 0\")\n",
    "    X_train = train.drop(\"sentiment\", axis=1)\n",
    "    X_train_os = train_os.drop(\"sentiment\", axis=1)\n",
    "    X_test = test.drop(\"sentiment\", axis=1)\n",
    "    y_train = train.sentiment.tolist()\n",
    "    y_train_os = train_os.sentiment.tolist()\n",
    "    y_test = test.sentiment.tolist()\n",
    "else:  # generate the files\n",
    "    data = pd.read_csv(os.path.join(BASE_DIR, \"data/raw/reviews.csv\"))\n",
    "    oversample_filepath = os.path.join(BASE_DIR, \"data/raw/reviews_oversample.csv\")\n",
    "    if not os.path.exists(\n",
    "            oversample_filepath\n",
    "    ):  # already generated, note: will take ~2hrs to generate the oversample\n",
    "        generate_oversample(raw_df=data, oversample_filepath=oversample_filepath)\n",
    "    data_os = pd.read_csv(oversample_filepath)\n",
    "    # without oversample\n",
    "    X_train, X_test, y_train, y_test = make_dataset(\n",
    "        data, train_split_output_filepath=train_filepath, test_split_output_filepath=test_filepath\n",
    "    )\n",
    "    # with oversample\n",
    "    print(\"Proceeding with FE on oversampled data...\")\n",
    "    X_train_os, X_test, y_train_os, y_test = make_dataset(\n",
    "        data_os,\n",
    "        train_split_output_filepath=train_oversample_filepath,\n",
    "        test_split_output_filepath=test_filepath,\n",
    "        oversample=True,\n",
    "    )\n",
    "\n",
    "if platform == \"win32\":\n",
    "    models_path = os.path.join(BASE_DIR, \"models\\\\sentiment_analysis\")\n",
    "else:\n",
    "    models_path = os.path.join(BASE_DIR, \"models/sentiment_analysis\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    models = {\n",
    "        \"xg_boost\": XgBoost(models_path),\n",
    "        \"xg_boost_svd\": XgBoostSvd(models_path),\n",
    "        \"log_reg\": LogReg(models_path),\n",
    "        \"svm\": SVM(models_path),\n",
    "        \"naive_bayes\": Naivebayes(models_path),\n",
    "        \"bert_fine_tuned\": BertFineTuned(models_path),\n",
    "        \"siebert\": Siebert(models_path),\n",
    "        \"lstm\": BasicLSTM(models_path),\n",
    "    }\n",
    "else:\n",
    "    models = {\n",
    "        \"xg_boost\": XgBoost(models_path),\n",
    "        \"xg_boost_svd\": XgBoostSvd(models_path),\n",
    "        \"log_reg\": LogReg(models_path),\n",
    "        \"svm\": SVM(models_path),\n",
    "        \"naive_bayes\": Naivebayes(models_path),\n",
    "        \"lstm\": BasicLSTM(models_path),\n",
    "    }\n",
    "\n",
    "# Train the models and save them\n",
    "train_models(models, X_train, y_train, X_train_os, y_train_os, models_path)\n",
    "best_model, best_model_name, best_accuracy = find_best_model(models, models_path, X_test, y_test)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Scoring Pipeline\n",
    "\n",
    "- Reads in the test file\n",
    "- Performs same preprocessing and feature engineering as above\n",
    "- Loads best model as determined by training pipeline\n",
    "- Runs prediction on test data\n",
    "- Returns csv with Text, Time, predicted_sentiment_probability, predicted_sentiment\n",
    "- Writes to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from src.models.sentiment_analysis.scoring_pipeline import run_scoring_pipeline\n",
    "\n",
    "# read in test csv which only has Time and Text columns\n",
    "test_data = pd.read_csv(\"data/raw/reviews_test.csv\")\n",
    "\n",
    "pred = run_scoring_pipeline(test_data)\n",
    "os.makedirs(\"data/predictions\", exist_ok=True)\n",
    "pred.to_csv(\"data/predictions/reviews_test_predictions_data-dialogue.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
