{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24266a16-c519-42d5-9bfd-8c5506d7aeae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e62d831-0e33-4e4d-9bb2-1cc83569610f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from src.data.feature_engineering import FeatureEngineer\n",
    "from src.data.preprocess import Preprocessor\n",
    "# Torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load Saved Model\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018632f4-b1f3-4cee-8885-4ce81d824726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set intial variables and constants\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Graph Designs\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# Random seed for reproducibilty\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ab5c4-b514-4b3b-839f-5ca076189cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a356d07-5520-427b-8155-2470a88d1f3d",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa43f06c-5742-4af7-87a9-26b0005fde85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c98275-345c-4ed3-b6de-85b6ee6c5911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] 102\n",
      "[CLS] 101\n",
      "[PAD] 0\n",
      "[UNK] 100\n"
     ]
    }
   ],
   "source": [
    "# Some of the common BERT tokens\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9318ad-5b30-4e19-a156-887beeb012be",
   "metadata": {},
   "source": [
    "Since BERT works with fixed length sequences, we need to store the length of each review and determine the max length of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25b22bf-8c7f-4bf0-8a23-927ed9913c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>Had Timing Objection</th>\n",
       "      <th>Timing Objection Index</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA77596bc061516d795f6a60fbd274cb0e</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  Hey, John. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAd8395ea9fec545909e633bba6a8eb643</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  Hey, Ivan. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAf15429ca373443cd6a6a88573fe16f98</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  Drake, this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA631c8faf6571f057e34bc12073da9f9c</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  File perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAbb4454527ef392d377ffd37a5bb00669</td>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  Hey, Sean. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>CAb5a575591cfbaf24ac936657c5befd47</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Sales Rep]  Hey, Mark. Skyler with NUCs. I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>CA84e33cc6a9aac5c8ecf41fe25e8be7ce</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>[[Prospect]  Hello?, [Sales Rep]  Maddie, hey....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>CAbfde17aa013e9896ece8102c8813f700</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Sales Rep]  Hello?, [Prospect]  Hello?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>CA16f2e7ff1215c1b52a509abd7c373732</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Prospect]  Hi. This is Brandon, [Sales Rep] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>CAa481788bab5e4fee93838fec4b49bd59</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Prospect]  Paul speaking., [Sales Rep]  Hey,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    SID  Had Timing Objection  \\\n",
       "0    CA77596bc061516d795f6a60fbd274cb0e                 False   \n",
       "1    CAd8395ea9fec545909e633bba6a8eb643                 False   \n",
       "2    CAf15429ca373443cd6a6a88573fe16f98                  True   \n",
       "3    CA631c8faf6571f057e34bc12073da9f9c                 False   \n",
       "4    CAbb4454527ef392d377ffd37a5bb00669                  True   \n",
       "..                                  ...                   ...   \n",
       "193  CAb5a575591cfbaf24ac936657c5befd47                 False   \n",
       "194  CA84e33cc6a9aac5c8ecf41fe25e8be7ce                  True   \n",
       "195  CAbfde17aa013e9896ece8102c8813f700                 False   \n",
       "196  CA16f2e7ff1215c1b52a509abd7c373732                 False   \n",
       "197  CAa481788bab5e4fee93838fec4b49bd59                 False   \n",
       "\n",
       "    Timing Objection Index                                         transcript  \n",
       "0                     None  [[Prospect]  Hello?, [Sales Rep]  Hey, John. H...  \n",
       "1                     None  [[Prospect]  Hello?, [Sales Rep]  Hey, Ivan. S...  \n",
       "2                        9  [[Prospect]  Hello?, [Sales Rep]  Drake, this ...  \n",
       "3                     None  [[Prospect]  Hello?, [Sales Rep]  File perform...  \n",
       "4                       35  [[Prospect]  Hello?, [Sales Rep]  Hey, Sean. I...  \n",
       "..                     ...                                                ...  \n",
       "193                   None  [[Sales Rep]  Hey, Mark. Skyler with NUCs. I w...  \n",
       "194                      9  [[Prospect]  Hello?, [Sales Rep]  Maddie, hey....  \n",
       "195                   None          [[Sales Rep]  Hello?, [Prospect]  Hello?]  \n",
       "196                   None  [[Prospect]  Hi. This is Brandon, [Sales Rep] ...  \n",
       "197                   None  [[Prospect]  Paul speaking., [Sales Rep]  Hey,...  \n",
       "\n",
       "[198 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the call data from the CSV file\n",
    "df = pd.read_csv('../../data/raw/call_data.csv')\n",
    "\n",
    "transcripts = {}\n",
    "\n",
    "# Iterate over the files in the transcripts directory\n",
    "for filename in os.listdir('../../data/raw/transcripts'):\n",
    "    # Get the SID from the filename\n",
    "    sid = os.path.splitext(filename)[0]\n",
    "    \n",
    "    with open(f'../../data/raw/transcripts/{filename}', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    lines = [re.sub(r'^\\d+.\\s', '', line.strip()) for line in lines]\n",
    "    transcripts[sid] = lines\n",
    "# Add the transcript data as a column to the call data\n",
    "df['transcript'] = df['SID'].map(transcripts)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5208ae-4240-42af-afab-4207d1ba0cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[Prospect]  Hello?, [Sales Rep]  Hey, John. H...\n",
       "1      [[Prospect]  Hello?, [Sales Rep]  Hey, Ivan. S...\n",
       "2      [[Prospect]  Hello?, [Sales Rep]  Drake, this ...\n",
       "3      [[Prospect]  Hello?, [Sales Rep]  File perform...\n",
       "4      [[Prospect]  Hello?, [Sales Rep]  Hey, Sean. I...\n",
       "                             ...                        \n",
       "193    [[Sales Rep]  Hey, Mark. Skyler with NUCs. I w...\n",
       "194    [[Prospect]  Hello?, [Sales Rep]  Maddie, hey....\n",
       "195            [[Sales Rep]  Hello?, [Prospect]  Hello?]\n",
       "196    [[Prospect]  Hi. This is Brandon, [Sales Rep] ...\n",
       "197    [[Prospect]  Paul speaking., [Sales Rep]  Hey,...\n",
       "Name: transcript, Length: 198, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc8752a-6b1f-4072-bb15-ab7ee7c8ab2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandarallel/core.py\", line 158, in __call__\n    results = self.work_function(\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandarallel/data_types/series.py\", line 26, in work\n    return data.apply(\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandas/core/series.py\", line 4771, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandas/core/apply.py\", line 1123, in apply\n    return self.apply_standard()\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandas/core/apply.py\", line 1174, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas/_libs/lib.pyx\", line 2924, in pandas._libs.lib.map_infer\n  File \"/Users/user/Dev/data-dialogue/src/data/preprocess.py\", line 265, in <lambda>\n    new_df[\"cleaned_transcript\"] = new_df[\"transcript\"].parallel_apply(lambda x: Preprocessor.clean_sentence(x, stopwords))\n  File \"/Users/user/Dev/data-dialogue/src/data/preprocess.py\", line 209, in clean_sentence\n    sentence = re.sub(r\"<.*?>|Length::\\d+:\\d+Mins\", \" \", sentence)  # remove tags\n  File \"/Users/user/miniforge3/envs/data-dialogue/lib/python3.9/re.py\", line 210, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m Preprocessor(df)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pre_processed_df \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mclean_df\n",
      "File \u001b[0;32m~/Dev/data-dialogue/src/data/preprocess.py:265\u001b[0m, in \u001b[0;36mPreprocessor.clean_csv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mClean the text data in the dirty_df DataFrame, adding a new column of cleaned text and a new column of\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03msentiment labels.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03mbinary 'Sentiment' column.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m new_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirty_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 265\u001b[0m new_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_transcript\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m new_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHad Timing Objection\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHad Timing Objection\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mparallel_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# lower case all column names\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-dialogue/lib/python3.9/site-packages/pandarallel/core.py:444\u001b[0m, in \u001b[0;36mparallelize_with_pipe.<locals>.closure\u001b[0;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m worker_status \u001b[38;5;241m==\u001b[39m WorkerStatus\u001b[38;5;241m.\u001b[39mError:\n\u001b[1;32m    442\u001b[0m         progress_bars\u001b[38;5;241m.\u001b[39mset_error(worker_index)\n\u001b[0;32m--> 444\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresults_promise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_type\u001b[38;5;241m.\u001b[39mreduce(results, reduce_extra)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-dialogue/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(df)\n",
    "preprocessor.clean_csv()\n",
    "pre_processed_df = preprocessor.clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1a593-447b-4488-82ee-385d1444f1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cbbee6-3027-476d-9c4f-a9ff35d19a6c",
   "metadata": {},
   "source": [
    "## Torch Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335b440-0e93-410a-9866-205019e1c3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    # Constructor Function \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # Length magic method\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    # get item magic method\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # Encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05147d19-113c-4088-9db9-b7e8606de783",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split data into training, test, validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679c431-450a-4a20-9050-e435beeff5c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train , df_test = train_test_split(df,\n",
    "#                                         test_size = 0.2,\n",
    "#                                         random_state = 4263)\n",
    "#\n",
    "# df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=4263)\n",
    "#\n",
    "# print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19343846",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read dataset from training pipeline\n",
    "df_train = pd.read_csv(\"../../data/processed/train_final_processed_reviews.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test = pd.read_csv(\"../../data/processed/test_final_processed_reviews.csv\", index_col=\"Unnamed: 0\")\n",
    "df_val = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb8226",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ff2c9-0b59-4ade-83b2-66c88daa6188",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loader to release data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003cf92-6c8d-4406-9591-f198300de43a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.text.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2ed33-ded6-43f9-b0ca-4078f7b41c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train, test and val data loaders\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db77f2f-3621-4fdc-8e44-ce4df3e29f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examples \n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1ccba-e4b4-40b6-a1f6-bcef068f6474",
   "metadata": {},
   "source": [
    "# Sentiment Classification with BERT and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a7570-cf59-463c-84f8-68e0a24ef700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8ef4b-f530-40f0-aa53-7af840bfe1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the Sentiment Classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # Forward propagaion class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "            , return_dict = False\n",
    "        )\n",
    "        #  Add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307589b6-d673-469d-b5b2-9c0df01d1f6d",
   "metadata": {},
   "source": [
    "We use a dropout layer for some regularization and a fully-connected layer for our output. We are returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work. Create an instance and move it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e137a-5645-4aad-8238-0bb8b15f2b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and move to classifier\n",
    "# 1: positive, 0: negative\n",
    "class_names = ['0','1']\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016b6ff-712e-4c86-b9f3-43a59c53438c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc7c91-6043-40aa-b0e6-fc4283581d18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae5bac-11c6-426c-815e-a24aa1a9ef57",
   "metadata": {},
   "source": [
    "AdamW optimizer provided by Hugging Face is used. It corrects weight decay. We’ll also use a linear scheduler with no warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e690b-2712-47a1-8e18-93a18a5d5028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 4\n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6c0fb-e660-4f5a-8b95-5c1019a1ebd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for a single training iteration\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            # , return_dict = False\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56caaa6f-9d86-452e-8ad6-9bdc4c7db0a2",
   "metadata": {},
   "source": [
    "## Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6a18f-8f39-4c7c-8f34-4928e9224104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "                # , return_dict = False\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597e920-9ccd-43c4-89c1-544c491ed494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce246d-a9ff-41bb-94e8-7a3046836e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387de63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "# plt.plot(history['train_acc'], label='train accuracy')\n",
    "# plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.plot(list(map(lambda x: x.cpu(),history['train_acc'])), label='train accuracy')\n",
    "plt.plot(list(map(lambda x: x.cpu(),history['val_acc'])), label='validation accuracy')\n",
    "\n",
    "# Graph chars\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018af681-007b-4e1d-9236-a3f91e373eab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Evaluation (For Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8de015-8651-4a38-8fcc-8561986507a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8571ffc-7e85-4035-b6c4-ff7eabf25733",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions (store text + predicted probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6f0db-62fb-4598-956b-b28d5b3440fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            # Get outputs\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303f6ba-88ed-4a0f-98ef-29e22467ecf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3a99e-4ea0-448b-afb9-5a3f063f2487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = ['0','1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f213f-ee8f-4692-b049-86cb65401326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9382b-a1a4-4847-9e91-9cafbe493ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a76b3e-5df5-45ae-86a4-7498e34fb9db",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03354a5-cf57-435c-819f-40d263910353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Model\n",
    "# from transformers import BertForSequenceClassification, BertConfig\n",
    "# MODEL_NAME = \"bert-base-cased\"\n",
    "# config = BertConfig.from_pretrained(MODEL_NAME)\n",
    "# model = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "# model = model.to(device)\n",
    "# model.load_state_dict(torch.load(\"best_model_state.bin\"),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5ded1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = 'bert_state_dict_new_clean.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73752bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save trained bert model\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de920213",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load trained bert model\n",
    "saved_model = SentimentClassifier(len(class_names))\n",
    "saved_model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "saved_model = saved_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1934ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  saved_model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae825493-2e56-4d1c-afd9-4ae124a9e358",
   "metadata": {},
   "source": [
    "# Model Evaluation (For saved model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d84fb-7c35-4bc9-8545-76680723276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            # Get outputs\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108be17-124b-4e9e-99f4-dbe30c3ce4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    saved_model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7bb76-3be8-4a16-8825-faa0f99aecea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbc233-0040-4b1f-8c6a-6a56b04f50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369add3-a2fb-4cb1-aeb1-4e4903da9f76",
   "metadata": {},
   "source": [
    "# Prediction on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f9202-ad8b-4926-8d24-09abd85262a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"I  like the dog food I bought,as it is healthy!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6d7f-f281-4950-8414-0b7ce52c7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbeb47-e421-4132-9184-66bbf890ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
