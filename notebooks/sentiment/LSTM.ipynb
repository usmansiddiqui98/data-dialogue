{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d7027-188e-4e12-bc07-adc67f38d7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import jovian\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43a183-f0b5-4469-bb1c-df5f1574b7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/cleaned_reviews.csv')\n",
    "df['labels'] = [1 if label =='positive' else 0 for label in df['Sentiment']] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d88872-f49b-4f28-afd1-61554c8a785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(df['clean_review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f7abe-e7b9-4373-b40a-297ab614f502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in nlp.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ca023-d208-4368-a01d-f4b3a0b58072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    counts.update(tokenize(row['clean_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e57d6-2004-4110-83c1-fe235b92f823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95418c7-8fa7-4f8d-a8a4-de9471ff8a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75400aee-1f32-4a7f-b7c4-405ed2039616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980b22a-5020-43e3-b702-d9990f390253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['encoded'] = df['clean_reviews'].apply(lambda x: np.array(encode_sentence(x,vocab2index), dtype = 'object'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdc3b9-9876-444f-8e09-9d0f6d41f17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = list(df['encoded'])\n",
    "y = list(df['labels'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state = 4263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d47774-fd5f-4b65-b4d8-368287fcb73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14d37f-44f0-408d-90e4-146e7087d097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bff10-611b-46fe-ae3c-f5f7a0ec1e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedb9b0-7004-465a-88fa-a48279aea648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a8bae-cc8e-41e6-840e-0253c2371c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"glove.6B/glove.6B.50d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411068f-5b40-4779-a374-99794967bc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3075c8-e017-40b5-bf70-fb6aa5289677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f4ee-8374-4705-999a-23d4c3f05269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966cd7c-02aa-4f4e-825d-6421479ba3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c10ca6-ea8b-4a77-b542-a31f0379e1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model(model, epochs=30, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685fcbd-a4fa-4cd0-9aac-8d4163dedab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e64ca-7859-4e14-9ba2-003ea1a9b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"The product does seem to be different from what is show on the webite, it is also slightly expensive, would not buy again\"\n",
    "predict_sentiment(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46226c55-318b-4343-a0ff-7bd33ce65860",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312c9d8-b7f7-49a7-988a-1aa6396b0601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b90e30-8982-42ff-ab14-f2c817cfe531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/clean_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85045a-07e1-4b36-999f-ca828b6933c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X,y = df['Cleaned Text'].values,df['Label'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y, random_state = 4263)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3279a-ac47-4217-9fd1-cafe9af42497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32df19-e89a-4b54-8fc8-7e14021db882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def create_word_list(x_train):\n",
    "    word_list = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split(' '):\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "    return word_list\n",
    "word_list = create_word_list(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aca968-8f43-48e3-888f-186252d06228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(x_train, y_train, x_test, y_test):\n",
    "    corpus = Counter(word_list)\n",
    "    corpus_ = sorted(corpus.items(), key = lambda x: x[1], reverse=True)[:2000]\n",
    "    onehot_dict = {w[0]:i+1 for i, w in enumerate(corpus_)}\n",
    "\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_test:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    # encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    # encoded_test = [1 if label =='positive' else 0 for label in y_test] \n",
    "    return np.array(final_list_train, dtype = 'object'), np.array(y_train),np.array(final_list_test, dtype = 'object'), np.array(y_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63264aa-7fff-43f2-9777-005b1091e45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tokenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d790c-ff2c-4930-8d4c-e4501df77c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83f298-d4da-4177-adf9-4aaa25fcf71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding(sents, seq_len):\n",
    "    features = np.zeros((len(sents), seq_len), dtype = int)\n",
    "    for i, rev in enumerate(sents):\n",
    "        if len(rev) != 0:\n",
    "            features[i, -len(rev):] = np.array(rev)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dc1e8-ca93-4e5f-b206-db552103300a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we have very less number of reviews of length > 30, so we will take review up till length 50 only\n",
    "x_train_pad = padding(x_train, 50)\n",
    "x_test_pad = padding(x_test, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d334e09-d6a2-48e2-8ba1-0f19028c87b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# batch size\n",
    "batch_size = 50\n",
    "\n",
    "# shuffle data\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size = batch_size, drop_last = True)\n",
    "test_loader = DataLoader(test_data, shuffle= True, batch_size = batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44514987-d581-4d60-a1c8-9259d4c5b5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "#     print(len(batch))\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    print(batch[0])\n",
    "    print(batch[1])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dd412-e599-4d57-9edb-4c3586e2717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain one batch\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print(f\"Size of Batch {sample_x.size()}\")\n",
    "print(f\"Sample input {sample_x}\")\n",
    "print(f\"Sample output {sample_y}\")\n",
    "# obtain one batch of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e97188-3881-4f4e-b889-0818d5128ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b692f-4fd7-4af8-85c4-74f0a45e796f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob = 0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.no_layers = no_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        \n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = self.hidden_dim, num_layers = no_layers, batch_first=True)\n",
    "        \n",
    "        #dropout layers\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        #linear and Sigmoid layer\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # we just passed a batch\n",
    "        batch_size = x.size(0) # batch size -> B\n",
    "        #embed shape -> [B, max_len, embed_dim]\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        \n",
    "        # drop out and fully connected\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid \n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        #reshape to batch size first\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        \n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # create hidden state and cell state tensors with size [no_layers x batch_size x hidden_dim]\n",
    "        \n",
    "        hidden_state = torch.zeros((self.no_layers, batch_size, self.hidden_dim))\n",
    "        cell_state = torch.zeros((self.no_layers, batch_size, self.hidden_dim))\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7744784-7dba-4143-bd47-c7c8ffc9299f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_layers = 2 # no of layers in lstm\n",
    "vocab_size = len(vocab) + 1 # extra for 0 (padding symbol)\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentLSTM(no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob = 0.5)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789871c4-daa6-46ca-84ee-492a4b86479e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimization features\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "# accuracy function\n",
    "def accuracy(pred, label):\n",
    "    pred =torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85de28b-7cae-4154-8037-5b10d9066960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 5\n",
    "test_loss_min = np.Inf\n",
    "epoch_tr_loss, epoch_tst_loss = [], []\n",
    "epoch_tr_acc, epoch_tst_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = 0\n",
    "    h = model.init_hidden(batch_size)\n",
    "    model.train()\n",
    "    # processing each batch\n",
    "    for x, y in train_loader:\n",
    "        x, y = x, y\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output, h = model(x, h)\n",
    "        \n",
    "        #calculate loss\n",
    "        \n",
    "        loss = criterion(output.squeeze(), y.float())\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        #acuracy\n",
    "        \n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "        train_acc += acc\n",
    "        #clip_Grad_norm clips the grad or simply prevents exploding of gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    test_h = model.init_hidden(batch_size)\n",
    "    test_loss = []\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    for x, y in test_loader:\n",
    "        x, y = x, y\n",
    "        test_h = tuple([each.data for each in test_h])\n",
    "        \n",
    "        output, test_h = model(x, test_h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), y.float())\n",
    "        test_loss.append(loss.item())\n",
    "        \n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "        test_acc += acc\n",
    "        \n",
    "    epoch_train_loss = np.mean(train_loss) # take average loss for each batch\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    \n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_tst_loss.append(epoch_test_loss)\n",
    "    \n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    \n",
    "    epoch_test_acc = test_acc / len(test_loader.dataset)\n",
    "    \n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_tst_acc.append(epoch_test_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} test_loss : {epoch_test_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} test_accuracy : {epoch_test_acc*100}')\n",
    "    if epoch_test_loss <= test_loss_min:\n",
    "        # torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(test_loss_min,epoch_test_loss))\n",
    "        test_loss_min = epoch_test_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31b7b5-8702-43fa-b650-57746867bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_tst_acc, label='Test Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_tst_loss, label='Test loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92b66d-ecff-4a96-b682-484c0e43a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() if preprocess_string(word) in vocab.keys()])\n",
    "    word_seq = np.expand_dims(word_seq, axis = 0)\n",
    "    print(word_seq)\n",
    "    pad = torch.from_numpy(padding(word_seq, 50))\n",
    "    \n",
    "    inputs = pad\n",
    "    batch_size = 1\n",
    "    h = model.init_hidden(batch_size)\n",
    "    output, h = model(inputs, h)\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b49bfc-0936-4e0a-8bab-69f4178f2549",
   "metadata": {},
   "source": [
    "Test on user generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b44bf-179f-4941-8d70-723e8da8146f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"The product does seem to be different from what is show on the webite, it is also slightly expensive, would not buy again\"\n",
    "pred = predict_sentiment(review)\n",
    "status = \"positive\" if pred > 0.5 else \"negative\"\n",
    "print(f\"Predicted Sentiment is {status} with probability of {pred}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21736cd0-9189-4735-bfe6-9ad82a32347e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"My kid does not like the baby powder at all even though it is highly recommended with good reviews\"\n",
    "pred = predict_sentiment(review)\n",
    "status = \"positive\" if pred > 0.5 else \"negative\"\n",
    "print(f\"Predicted Sentiment is {status} with probability of {pred}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547efd1-8294-4623-98aa-742be188e8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"My pet dog could not get used to the kibbles and vomited everything out\"\n",
    "pred = predict_sentiment(review)\n",
    "status = \"positive\" if pred > 0.5 else \"negative\"\n",
    "print(f\"Predicted Sentiment is {status} with probability of {pred}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f5541-c878-404f-b294-35cc25e7fd88",
   "metadata": {
    "tags": []
   },
   "source": [
    "19 March 2023\n",
    "\n",
    "26% of data has negative sentiments\n",
    "74% of data has positive sentiments\n",
    "\n",
    "Data is slightly unbalanced and as seen from the model, the predicted probability for negative sentiment seems to be quite low. The model is good at classifying positive reviews but can be better on negative ones.\n",
    "\n",
    "Things to do next:\n",
    "- hyperparameter tuning \n",
    "- can tune threshold in which we determine whether sentiment is +/-, currently it is at 0.5\n",
    "- try other models such as XGboost which is great for unbalanced dataset\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6d290-1e1b-40be-9684-8a407dfeba03",
   "metadata": {
    "tags": []
   },
   "source": [
    "20 March 2023\n",
    "\n",
    "- reduce the length of reviews but changing the padding amount from 500 to 50 as most of the reviews are very short, this proved to improve the model significantly as the training/testing loss and accuracy improved and there is lesser overfitting\n",
    "\n",
    "- Model highest train accuracy is 88% and test accuracy is 82%\n",
    "\n",
    "- tune the threshold such that probability of >0.5 does not neccessarily mean positive\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3d2c2-92e2-4c11-b39a-bbee73787bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b94cd-3601-47b2-8faf-0c0e363e9b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/clean_reviews.csv')\n",
    "df['label'] = [1 if label =='positive' else 0 for label in df['Sentiment'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62e986-2bf4-4aab-8866-04d19c623bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test \n",
    "    = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    " X_train, X_val, y_train, y_val \n",
    "    = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e303b5-f486-4092-a554-3df7bf9f38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
    "fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
    "\n",
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "# Vocabulary\n",
    "\n",
    "text_field.build_vocab(train, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2541db-8f35-43b4-a05b-a5e219a0cb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torchtext==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415676c-c010-4fc5-8e17-e81c7924476b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1eab87-96c3-4c6b-ab08-c5b681ea50c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84607bb4-6b34-4425-9896-7ad3e8a2d388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa428c-207a-49a5-bc14-b18e70d63e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiLSTM_SentimentAnalysis(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout) :\n",
    "        super().__init__()\n",
    "\n",
    "        # The embedding layer takes the vocab size and the embeddings size as input\n",
    "        # The embeddings size is up to you to decide, but common sizes are between 50 and 100.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM layer takes in the the embedding size and the hidden vector size.\n",
    "        # The hidden dimension is up to you to decide, but common values are 32, 64, 128\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # We use dropout before the final layer to improve with regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # The fully-connected layer takes in the hidden dim of the LSTM and\n",
    "        #  outputs a a 3x1 vector of the class scores.\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        The forward method takes in the input and the previous hidden state \n",
    "        \"\"\"\n",
    "\n",
    "        # The input is transformed to embeddings by passing it to the embedding layer\n",
    "        embs = self.embedding(x)\n",
    "\n",
    "        # The embedded inputs are fed to the LSTM alongside the previous hidden state\n",
    "        out, hidden = self.lstm(embs, hidden)\n",
    "\n",
    "        # Dropout is applied to the output and fed to the FC layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # We extract the scores for the final hidden state since it is the one that matters.\n",
    "        out = out[:, -1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.tensor(torch.zeros(1, batch_size, 32)),torch.tensor(torch.zeros(1, batch_size, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6957745-2b56-4101-926a-0e6fc1209ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_SentimentAnalysis(len(vocab), 64, 32, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1ee04-32c0-4791-8bec-7acef6e01906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb79e57-9667-46f5-aec3-3ff39eda49c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "\n",
    "    h0, c0 =  model.init_hidden()\n",
    "    print(h0)\n",
    "    print(c0)\n",
    "\n",
    "    # h0 = h0.to(device)\n",
    "    # c0 = c0.to(device)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        # input = batch[0].to(device)\n",
    "        # target = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            out, hidden = model(input, (h0, c0))\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d31826-12d9-4886-ba0c-c0b5eee1795a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcac9b-c34a-44bf-9216-5fa9cfb0faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_acc = []\n",
    "for batch_idx, batch in enumerate(test_dl):\n",
    "\n",
    "    input = batch[0].to(device)\n",
    "    target = batch[1].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        out, hidden = model(input, (h0, c0))\n",
    "        _, preds = torch.max(out, 1)\n",
    "        preds = preds.to(\"cpu\").tolist()\n",
    "        batch_acc.append(accuracy_score(preds, target.tolist()))\n",
    "\n",
    "sum(batch_acc)/len(batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423a2f7-1d2f-42c8-a9a7-2da48c7c793f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c4a7d-5d30-4f7a-897a-976ff328d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
