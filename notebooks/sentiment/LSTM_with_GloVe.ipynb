{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e812a1-4336-4470-8ebb-ca0cc2e07a3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Bi-directional LSTM with GloVe, ANN to predict sentiment of topic and LSTM to predict sentiment of text, finally models ensembled to make final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1952aa-f897-411d-9833-a2d973c9d92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import string\n",
    "import spacy\n",
    "import jovian\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1b323-b7dc-474a-9f7a-b556546347cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_embedding_dimension = 200\n",
    "key_embedding_dimension = 50\n",
    "\n",
    "\n",
    "path_to_glove_file = \"../../data/embeddings/glove.6B/glove.6B.200d.txt\".format(text_embedding_dimension)\n",
    "\n",
    "embeddings_index_200 = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index_200[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index_200))\n",
    "\n",
    "path_to_glove_file = \"../../data/embeddings/glove.6B/glove.6B.50d.txt\".format(key_embedding_dimension)\n",
    "\n",
    "embeddings_index_25 = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index_25[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4c79a-f837-4f44-8f52-f1f5d05d10f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/clean_reviews_w_topics.csv')\n",
    "df['labels'] = [1 if label =='positive' else 0 for label in df['sentiment']] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637f7b8-7173-45f0-a71a-869147773690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b39004-cf03-4b51-8034-7ffe58684d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknown_words = []\n",
    "total_words = 0\n",
    "\n",
    "def find_unknown_words(words):\n",
    "    \n",
    "    global total_words\n",
    "    total_words = total_words + len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if not (word in embeddings_index_200):\n",
    "            unknown_words.append(word)\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "df['cleaned_text'].apply(lambda words: find_unknown_words(words))\n",
    "\n",
    "print( f'{len(unknown_words)/total_words*100:5.2} % of words are unknown' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ae05c-46e4-474b-85c6-d58e1ccca296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_unknown_words(unknown_words):\n",
    "    \n",
    "    unknown_words = np.array(unknown_words)\n",
    "    (word, count) = np.unique(unknown_words, return_counts=True)\n",
    "    \n",
    "    word_freq = pd.DataFrame({'word': word, 'count': count}).sort_values('count', ascending=False)\n",
    "\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "          header=dict(values=list(word_freq.columns),\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'),\n",
    "          cells=dict(values=[word_freq['word'], word_freq['count']],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'))\n",
    "          ])\n",
    "    fig.update_layout(width=300, height=300, margin={'b':0, 'l':0, 'r':0, 't':0, 'pad':0})\n",
    "    fig.show()\n",
    "        \n",
    "analyze_unknown_words(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0fb012-8a03-45ef-a6cf-71ce7d2179ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_embed(words):\n",
    "    \n",
    "    unknown_indices = []\n",
    "    mean = np.zeros(text_embedding_dimension)\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if words[i] in embeddings_index_200:\n",
    "            words[i] = embeddings_index_200[ words[i] ]\n",
    "            mean += words[i]\n",
    "        else:\n",
    "            unknown_indices.append(i)\n",
    "            \n",
    "    mean /= len(words)-len(unknown_indices)\n",
    "    \n",
    "    # unknown words in the text are represented using the mean of the known words\n",
    "    for i in unknown_indices:\n",
    "        words[i] = mean\n",
    "    \n",
    "    return np.array(words)\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda words: text_embed(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bde83-a881-4234-b83b-71003df27603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keyword_embed(keyword, text):\n",
    "    \n",
    "    if pd.isna(keyword):\n",
    "        keyword = np.zeros(50)\n",
    "    else:\n",
    "        keyword = keyword.lower()\n",
    "        keyword = re.sub(\"[^a-z ]+\", ' ', keyword)\n",
    "        keywords = keyword.split()\n",
    "\n",
    "        if len(keywords) == 0:\n",
    "            keyword = np.zeros(key_embedding_dimension)\n",
    "        else:\n",
    "            keyword = np.zeros(key_embedding_dimension)\n",
    "            word_count = 0\n",
    "            for word in keywords:\n",
    "                if word in embeddings_index_25:\n",
    "                    # print(keyword)\n",
    "                    # print(embeddings_index_25[word])\n",
    "                    keyword += embeddings_index_25[word]\n",
    "                    word_count += 1\n",
    "\n",
    "            if word_count > 0:\n",
    "                keyword = keyword / word_count\n",
    " \n",
    "    return keyword\n",
    "\n",
    "df['word_1'] = df.apply(lambda x: keyword_embed(x['word_1'], x['cleaned_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf79c0-733a-48b8-91d1-192f2bc86e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_validation_ratio = 0.2\n",
    "# cross_validation_ratio = 0.05\n",
    "\n",
    "mask = np.random.rand(len(df)) > cross_validation_ratio\n",
    "\n",
    "train_df = df[mask]\n",
    "\n",
    "val_df = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17b81d-9af7-4c99-b9ce-cff543e14ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_text = train_df['cleaned_text'].values\n",
    "x_train_key = train_df['word_1'].values\n",
    "\n",
    "x_val_text = val_df['cleaned_text'].values\n",
    "x_val_key = val_df['word_1'].values\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b58d49-c5d7-4536-a00f-83573e21b919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_key = np.array( [i for i in x_train_key] ).reshape(-1, key_embedding_dimension)\n",
    "x_val_key = np.array( [i for i in x_val_key] ).reshape(-1, key_embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7cb59-c6da-458b-9c96-7bcb454ede5f",
   "metadata": {},
   "source": [
    "ANN to predict sentiment of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5ce9c-4cb2-49ef-b20e-c9391a6816b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()                          \n",
    "        self.fc1 = nn.Linear(key_embedding_dimension, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.fc1(X)\n",
    "        X = self.bn1(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.dropout1(X)\n",
    "        X = self.fc2(X)\n",
    "        X = torch.sigmoid(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a89c24-bb9c-4c9c-b87b-6009f4325471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_model = ANN_Model()\n",
    "\n",
    "# if cuda0 != None:\n",
    "#   ann_model.to(cuda0)\n",
    "\n",
    "criterion_key = nn.BCELoss()\n",
    "optimizer_key = torch.optim.Adam(ann_model.parameters(), lr=0.01)\n",
    "# scheduler_key = torch.optim.lr_scheduler.ExponentialLR(optimizer_key, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8169d-e797-4fc7-9982-2c709ae0084d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ef774-8903-4784-952a-efaba1cfe2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(300):  \n",
    "    \n",
    "    ann_model.train()\n",
    "\n",
    "    tweet = torch.FloatTensor(x_train_key)\n",
    "    label = torch.FloatTensor(y_train)\n",
    "\n",
    "    # if cuda0 != None:\n",
    "    #     tweet = tweet.cuda()\n",
    "    #     label = label.cuda()\n",
    "\n",
    "    pred = ann_model(tweet)\n",
    "    pred = pred.reshape(-1)\n",
    "\n",
    "    loss = criterion_key(pred, label)\n",
    "\n",
    "    optimizer_key.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_key.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    train_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() / len(x_train_key) )\n",
    "    \n",
    "    ann_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tweet = torch.FloatTensor(x_val_key)\n",
    "        label = torch.FloatTensor(y_val)\n",
    "\n",
    "        # if cuda0 != None:\n",
    "        #     tweet = tweet.cuda()\n",
    "        #     label = label.cuda()\n",
    "\n",
    "        pred = ann_model(tweet)\n",
    "        pred = pred.reshape(-1)\n",
    "\n",
    "        loss = criterion_key(pred, label)\n",
    "\n",
    "    val_losses.append(loss.item())\n",
    "    val_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() / len(x_val_key) )\n",
    "    \n",
    "    if (epoch+1)%50 == 0:\n",
    "        print('Epoch {} Summary:'.format(epoch+1))\n",
    "        print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n",
    "        print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n",
    "        print('')\n",
    "\n",
    "    # scheduler_key.step()\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa522d-ad9d-424a-81d0-63b09d9db40c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_axis = [i+1 for i in range(len(train_losses))]\n",
    "\n",
    "plt.plot(x_axis, train_losses, label='training loss')\n",
    "plt.plot(x_axis, val_losses, label='validation loss')\n",
    "plt.title('Loss for each epoch')\n",
    "plt.legend();\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_axis, train_accuracies, label='training accuracy')\n",
    "plt.plot(x_axis, val_accuracies, label='validation accuracy')\n",
    "plt.title('Accuracy for each epoch')\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa23237-b4fc-4e0c-ac12-31b7d917fb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_model.eval()\n",
    "\n",
    "# predictions for the training set\n",
    "with torch.no_grad():\n",
    "\n",
    "    tweet = torch.FloatTensor(x_train_key)\n",
    "\n",
    "    # if cuda0 != None:\n",
    "    #     tweet = tweet.cuda()\n",
    "\n",
    "    pred_train_key = ann_model(tweet)\n",
    "    pred_train_key = pred_train_key.reshape(-1)\n",
    "    \n",
    "\n",
    "# predictions for the cross validation set\n",
    "with torch.no_grad():\n",
    "\n",
    "    tweet = torch.FloatTensor(x_val_key)\n",
    "\n",
    "#     if cuda0 != None:\n",
    "#         tweet = tweet.cuda()\n",
    "\n",
    "    pred_val_key = ann_model(tweet)\n",
    "    pred_val_key = pred_val_key.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7a8dd-9cec-4967-8e80-9a9c9ae32cd5",
   "metadata": {},
   "source": [
    "LSTM to predict sentiment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b4664-4e0b-4ea4-9c3c-5053b9af3b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMnetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.input_size = text_embedding_dimension\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 1\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.num_directions = 2\n",
    " \n",
    "        self.lstm = nn.LSTM( self.input_size, self.hidden_size, self.num_layers, \n",
    "                             bidirectional=self.bidirectional )\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size*self.num_directions,1)\n",
    "\n",
    "    def forward(self, tweet):\n",
    "        \n",
    "        lstm_out, _ = self.lstm( tweet.view(len(tweet), 1, -1) )\n",
    "\n",
    "        x = self.dropout1( lstm_out.view(len(tweet),-1) )\n",
    "        \n",
    "        output = self.linear(x)\n",
    "        \n",
    "        pred = torch.sigmoid( output[-1] )\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e795e9-2ea9-432b-9865-66263f37677b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTMnetwork()\n",
    "\n",
    "# if cuda0 != None:\n",
    "#   lstm_model.to(cuda0)\n",
    "\n",
    "criterion_text = nn.BCELoss()\n",
    "optimizer_text = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "scheduler_text = torch.optim.lr_scheduler.ExponentialLR(optimizer_text, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607425e6-b025-499b-8c8b-aa51cd30f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe10ce9-e0b6-46de-bedd-02b508944480",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final prediction using only LSTM (better results than combining both ANN and LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074be692-9303-4ada-8aa9-bd639c85a390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(10):  \n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    print('Epoch : {}'.format(epoch+1))\n",
    "\n",
    "    trainLoss = 0\n",
    "    correct = 0\n",
    "\n",
    "    lstm_model.train()\n",
    "\n",
    "    for i in range(len(x_train_text)):\n",
    "\n",
    "        lstm_model.zero_grad()\n",
    "\n",
    "        tweet = torch.FloatTensor(x_train_text[i])\n",
    "        label = torch.FloatTensor( np.array([y_train[i]]) )\n",
    "\n",
    "        # if cuda0 != None:\n",
    "        #     tweet = tweet.cuda()\n",
    "        #     label = label.cuda()\n",
    "\n",
    "        pred = lstm_model(tweet)\n",
    "        \n",
    "        loss = criterion_text(pred, label)\n",
    "\n",
    "        lambdaParam = torch.tensor(0.001)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # if cuda0 != None:\n",
    "        #   lambdaParam = lambdaParam.cuda()\n",
    "        #   l2_reg = l2_reg.cuda() \n",
    "\n",
    "        for param in lstm_model.parameters():\n",
    "          # if cuda0 != None:\n",
    "          #   l2_reg += torch.norm(param).cuda()\n",
    "          # else:\n",
    "            l2_reg += torch.norm(param)\n",
    "\n",
    "        loss += lambdaParam * l2_reg\n",
    "\n",
    "        pred = pred.item()\n",
    "        # *lstm_model_weight + pred_train_key[i].item()*ann_model_weight\n",
    "        \n",
    "        if pred > 0.5:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "\n",
    "        if pred == int( label.item() ):\n",
    "            correct += 1\n",
    "\n",
    "        trainLoss += loss.item()\n",
    "\n",
    "        optimizer_text.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_text.step()\n",
    "\n",
    "        if (i+1)%1000 == 0:\n",
    "            print('Processed {} tweets out of {}'.format(i+1, len(x_train_text)))\n",
    "\n",
    "    train_losses.append(trainLoss/len(x_train_text))\n",
    "    train_accuracies.append( correct/len(x_train_text) )\n",
    "\n",
    "    valLoss = 0\n",
    "    correct = 0\n",
    "\n",
    "    lstm_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(len(x_val_text)):\n",
    "\n",
    "            tweet = torch.FloatTensor(x_val_text[i])\n",
    "            label = torch.FloatTensor( np.array([y_val[i]]) )\n",
    "\n",
    "            # if cuda0 != None:\n",
    "            #     tweet = tweet.cuda()\n",
    "            #     label = label.cuda()\n",
    "\n",
    "            pred = lstm_model( tweet )\n",
    "\n",
    "            loss = criterion_text(pred, label)\n",
    "\n",
    "            valLoss += loss.item()\n",
    "\n",
    "            pred = pred.item()\n",
    "            # *lstm_model_weight + pred_val_key[i].item()*ann_model_weight\n",
    "\n",
    "            if pred > 0.5:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "            if pred == int( label.item() ):\n",
    "                correct += 1\n",
    "\n",
    "    val_losses.append(valLoss/len(x_val_text))\n",
    "    val_accuracies.append( correct/len(x_val_text) )\n",
    "\n",
    "    print('Epoch Summary:')\n",
    "    print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n",
    "    print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n",
    "    print(f'Duration: {time.time() - epoch_start_time:.0f} seconds')\n",
    "    print('')\n",
    "\n",
    "    scheduler_text.step()\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca192e-0f45-4ce2-8618-ceb070df0a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_axis = [i+1 for i in range(len(train_losses))]\n",
    "\n",
    "plt.plot(x_axis, train_losses, label='training loss')\n",
    "plt.plot(x_axis, val_losses, label='validation loss')\n",
    "plt.title('Loss for each epoch')\n",
    "plt.legend();\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_axis, train_accuracies, label='training accuracy')\n",
    "plt.plot(x_axis, val_accuracies, label='validation accuracy')\n",
    "plt.title('Accuracy for each epoch')\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b5414-d27a-451d-b77b-24841e8d6d5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final prediction using both ANN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663c86f-dd96-47cb-868b-70a87b68e79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_model_weight = 0.1\n",
    "lstm_model_weight = 1-ann_model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee48c8-aa22-4be5-9d5b-39da013e9066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(10):  \n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    print('Epoch : {}'.format(epoch+1))\n",
    "\n",
    "    trainLoss = 0\n",
    "    correct = 0\n",
    "\n",
    "    lstm_model.train()\n",
    "\n",
    "    for i in range(len(x_train_text)):\n",
    "\n",
    "        lstm_model.zero_grad()\n",
    "\n",
    "        tweet = torch.FloatTensor(x_train_text[i])\n",
    "        label = torch.FloatTensor( np.array([y_train[i]]) )\n",
    "\n",
    "        # if cuda0 != None:\n",
    "        #     tweet = tweet.cuda()\n",
    "        #     label = label.cuda()\n",
    "\n",
    "        pred = lstm_model(tweet)\n",
    "        \n",
    "        loss = criterion_text(pred, label)\n",
    "\n",
    "        lambdaParam = torch.tensor(0.001)\n",
    "        l2_reg = torch.tensor(0.)\n",
    "\n",
    "        # if cuda0 != None:\n",
    "        #   lambdaParam = lambdaParam.cuda()\n",
    "        #   l2_reg = l2_reg.cuda() \n",
    "\n",
    "        for param in lstm_model.parameters():\n",
    "          # if cuda0 != None:\n",
    "          #   l2_reg += torch.norm(param).cuda()\n",
    "          # else:\n",
    "            l2_reg += torch.norm(param)\n",
    "\n",
    "        loss += lambdaParam * l2_reg\n",
    "\n",
    "        pred = pred.item()*lstm_model_weight + pred_train_key[i].item()*ann_model_weight\n",
    "        \n",
    "        if pred > 0.5:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "\n",
    "        if pred == int( label.item() ):\n",
    "            correct += 1\n",
    "\n",
    "        trainLoss += loss.item()\n",
    "\n",
    "        optimizer_text.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_text.step()\n",
    "\n",
    "        if (i+1)%1000 == 0:\n",
    "            print('Processed {} tweets out of {}'.format(i+1, len(x_train_text)))\n",
    "\n",
    "    train_losses.append(trainLoss/len(x_train_text))\n",
    "    train_accuracies.append( correct/len(x_train_text) )\n",
    "\n",
    "    valLoss = 0\n",
    "    correct = 0\n",
    "\n",
    "    lstm_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(len(x_val_text)):\n",
    "\n",
    "            tweet = torch.FloatTensor(x_val_text[i])\n",
    "            label = torch.FloatTensor( np.array([y_val[i]]) )\n",
    "\n",
    "            # if cuda0 != None:\n",
    "            #     tweet = tweet.cuda()\n",
    "            #     label = label.cuda()\n",
    "\n",
    "            pred = lstm_model( tweet )\n",
    "\n",
    "            loss = criterion_text(pred, label)\n",
    "\n",
    "            valLoss += loss.item()\n",
    "\n",
    "            pred = pred.item()*lstm_model_weight + pred_val_key[i].item()*ann_model_weight\n",
    "\n",
    "            if pred > 0.5:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "            if pred == int( label.item() ):\n",
    "                correct += 1\n",
    "\n",
    "    val_losses.append(valLoss/len(x_val_text))\n",
    "    val_accuracies.append( correct/len(x_val_text) )\n",
    "\n",
    "    print('Epoch Summary:')\n",
    "    print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n",
    "    print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n",
    "    print(f'Duration: {time.time() - epoch_start_time:.0f} seconds')\n",
    "    print('')\n",
    "\n",
    "    scheduler_text.step()\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e69cbd-7a63-40a1-a8db-f51a4debd1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_axis = [i+1 for i in range(len(train_losses))]\n",
    "\n",
    "plt.plot(x_axis, train_losses, label='training loss')\n",
    "plt.plot(x_axis, val_losses, label='validation loss')\n",
    "plt.title('Loss for each epoch')\n",
    "plt.legend();\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_axis, train_accuracies, label='training accuracy')\n",
    "plt.plot(x_axis, val_accuracies, label='validation accuracy')\n",
    "plt.title('Accuracy for each epoch')\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cfa0a-1832-4af7-99e7-106b4f168797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simple LSTM with GloVe Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43a183-f0b5-4469-bb1c-df5f1574b7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/cleaned_reviews.csv')\n",
    "df['labels'] = [1 if label =='positive' else 0 for label in df['Sentiment']] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d88872-f49b-4f28-afd1-61554c8a785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(df['clean_review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f7abe-e7b9-4373-b40a-297ab614f502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in nlp.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ca023-d208-4368-a01d-f4b3a0b58072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    counts.update(tokenize(row['clean_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e57d6-2004-4110-83c1-fe235b92f823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95418c7-8fa7-4f8d-a8a4-de9471ff8a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75400aee-1f32-4a7f-b7c4-405ed2039616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980b22a-5020-43e3-b702-d9990f390253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['encoded'] = df['clean_reviews'].apply(lambda x: np.array(encode_sentence(x,vocab2index), dtype = 'object'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdc3b9-9876-444f-8e09-9d0f6d41f17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = list(df['encoded'])\n",
    "y = list(df['labels'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state = 4263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d47774-fd5f-4b65-b4d8-368287fcb73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14d37f-44f0-408d-90e4-146e7087d097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bff10-611b-46fe-ae3c-f5f7a0ec1e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedb9b0-7004-465a-88fa-a48279aea648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a8bae-cc8e-41e6-840e-0253c2371c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"../../data/embeddings/glove.6B/glove.6B.50d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411068f-5b40-4779-a374-99794967bc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3075c8-e017-40b5-bf70-fb6aa5289677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f4ee-8374-4705-999a-23d4c3f05269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966cd7c-02aa-4f4e-825d-6421479ba3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c10ca6-ea8b-4a77-b542-a31f0379e1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model(model, epochs=30, lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
