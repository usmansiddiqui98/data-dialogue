{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "from src.models.topic_modelling.LDA import LDAGensim\n",
    "from src.models.topic_modelling.LSA import LSAModel\n",
    "from src.data.preprocess import Preprocessor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(\"../../data/raw/reviews.csv\")\n",
    "preprocessor.clean_csv()\n",
    "df = preprocessor.clean_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/cleaned_reviews.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def freq_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "    # selecting top 20 most frequent words\n",
    "    d = words_df.nlargest(columns=\"count\", n = terms)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "freq_words(df['clean_reviews'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_reviews = pd.Series(df['clean_reviews']).apply(lambda x: x.split())\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def lemmatization(texts, tags):# filter based on tags\n",
    "    output = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter text based on nouns and adjectives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews_2 = lemmatization(tokenized_reviews, tags =['NOUN', 'ADJ'] )\n",
    "print(reviews_2[1]) # print lemmatized review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "freq_words([item for sublist in reviews_2 for item in sublist])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter text based on nouns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews_3 = lemmatization(tokenized_reviews, tags =['NOUN'] )\n",
    "print(reviews_3[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "freq_words([item for sublist in reviews_3 for item in sublist])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter text based on adjectives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews_4 = lemmatization(tokenized_reviews, tags =['ADJ'] )\n",
    "print(reviews_4[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "freq_words([item for sublist in reviews_4 for item in sublist])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSA modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LSAmodel(data, no_of_topics):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "    max_features= 1000, # keep top 1000 terms\n",
    "    max_df = 0.5,\n",
    "    smooth_idf=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    print(X.shape)\n",
    "    svd_model = TruncatedSVD(n_components=no_of_topics, algorithm='randomized', n_iter=100, random_state=122)\n",
    "    svd_model.fit(X)\n",
    "    print(len(svd_model.components_))\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        sentence = \"\"\n",
    "        print(sorted_terms)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSA without pre-processing (using df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(df['clean_reviews'],10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSA with processed_reviews (both lemmatized nouns + adjs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adj_nouns_reviews = [item for sublist in reviews_2 for item in sublist]\n",
    "LSAmodel(adj_nouns_reviews,5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(adj_nouns_reviews,4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(adj_nouns_reviews,3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSA with processed_reviews (lemmatized nouns only)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nouns_reviews = [item for sublist in reviews_3 for item in sublist]\n",
    "LSAmodel(nouns_reviews,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(nouns_reviews,5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(nouns_reviews,4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LSAmodel(nouns_reviews,3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSA with processed_reviews (lemmatized adjs only)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adj_reviews = [item for sublist in reviews_4 for item in sublist]\n",
    "LSAmodel(adj_reviews,10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSA assumes a Gaussian distribution of the terms in the documents, which may not be true for all problems."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA (1) using sklearn (do not run as the hyperparameter tuning takes very long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lda_sklearn(reviews, numtopics):\n",
    "    # create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "    # fit and transform the clean text data\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "    # Materialize the sparse data\n",
    "    data_dense = X.todense()\n",
    "\n",
    "    # Compute Sparsicity\n",
    "    # Sparsicity is the percentage of non-zero datapoints in X\n",
    "    print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "    # create an LDA object and fit the data\n",
    "    lda = LatentDirichletAllocation(n_components=numtopics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # print the top words in each topic\n",
    "    feature_names = sorted(vectorizer.vocabulary_.keys())\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        topic_complete = (\", \".join([feature_names[i] for i in topic.argsort()[:-15:-1]]))\n",
    "        print(topic_complete)\n",
    "        topic_list.append(topic_complete)\n",
    "\n",
    "    # Log Likelyhood: Higher the better\n",
    "    print(\"Log Likelihood: \", lda.score(X))\n",
    "\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Perplexity: \", lda.perplexity(X))\n",
    "\n",
    "    # See model parameters\n",
    "    pprint.pprint(lda.get_params())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reviews_2 consist of both nouns and adjs only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reviews_2 consist of both nouns and adjs only\n",
    "adj_nouns_reviews = [item for sublist in reviews_2 for item in sublist]\n",
    "lda_sklearn(adj_nouns_reviews,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reviews_3 consist of both nouns only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nouns_reviews = [item for sublist in reviews_3 for item in sublist]\n",
    "lda_sklearn(nouns_reviews,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def search_best_model(reviews):\n",
    "    # create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "    # fit and transform the clean text data\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [3,4,5,6,7], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(X)\n",
    "\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(X))\n",
    "\n",
    "    # Get Log Likelyhoods from Grid Search Output\n",
    "    n_topics = [3,4,5,6,7,8]\n",
    "    log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "    log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "    log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "    # Show graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "    plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "    plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "    plt.title(\"Choosing Optimal LDA Model\")\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Log Likelyhood Scores\")\n",
    "    plt.legend(title='Learning decay', loc='best')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_best_model(adj_nouns_reviews)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_best_model(nouns_reviews)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good.\n",
    "On a different note, perplexity might not be the best measure to evaluate topic models because it doesn’t consider the context and semantic associations between words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be concluded that hyperparameter tuning has not been effective"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA (2) using gensim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lda_gensim(cleaned_reviews, num_topics):\n",
    "    # create the id2word dictionary\n",
    "    id2word = corpora.Dictionary(cleaned_reviews)\n",
    "\n",
    "    # create the corpus\n",
    "    corpus = [id2word.doc2bow(tokens) for tokens in cleaned_reviews]\n",
    "\n",
    "    # create the LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=10)\n",
    "    for element in lda_model.print_topics():\n",
    "        print('Topic ' + str(element[0]))\n",
    "        print(element[1])\n",
    "    return lda_model, corpus, id2word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lda_viz(lda_model, corpus, id2word):\n",
    "    # visualize the topics using pyLDAvis\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    return vis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reviews_1 refers to full text\n",
    "reviews_1 = []\n",
    "for review in df['clean_reviews']:\n",
    "    reviews_1.append(word_tokenize(review))\n",
    "\n",
    "model, corpus, id2_word = lda_gensim(reviews_1, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word = lda_gensim(reviews_1, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word = lda_gensim(reviews_1, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reviews_2 consist of both nouns and adjs only\n",
    "model, corpus, id2_word = lda_gensim(reviews_2, 6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word = lda_gensim(reviews_2, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word = lda_gensim(reviews_2, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word = lda_gensim(reviews_2, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reviews_3 consist of nouns only\n",
    "model, corpus, id2_word =lda_gensim(reviews_3, 6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_3, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_3, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_3, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reviews_4 consist of adjs only\n",
    "model, corpus, id2_word =lda_gensim(reviews_4, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_4, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_4, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusion:\n",
    "From using texts filtered on nouns only, We can narrow down texts in to 6 main topics:\n",
    "- Pets\n",
    "- Baby\n",
    "- Snacks\n",
    "- Beverages\n",
    "- Protein/Food\n",
    "- Condiments/Products"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, corpus, id2_word =lda_gensim(reviews_3, 6) # texts based on nouns only\n",
    "lda_viz(model, corpus, id2_word )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NER (Named Entity Recognition)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pet products"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_pet_entities(text):\n",
    "    doc = nlp(text)\n",
    "    pet_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"ANIMAL\" or \"pet\" in entity.text.lower() or \"dog\" in entity.text.lower() or \"cat\" in entity.text.lower():\n",
    "            pet_entities.append(entity.text)\n",
    "    return pet_entities\n",
    "\n",
    "# apply the extract_pet_entities function to the reviews column\n",
    "df['pet_entities'] = df['clean_reviews'].apply(extract_pet_entities)\n",
    "\n",
    "# print the unique pet-related entities that were extracted\n",
    "pet_entities = set([entity for row in df['pet_entities'] for entity in row])\n",
    "print(pet_entities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(pet_entities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(['pet_entities'], inplace = True, axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_product_entities(text):\n",
    "    doc = nlp(text)\n",
    "    product_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"PRODUCT\" or \"coffee\" in entity.text.lower() or \"tea\" in entity.text.lower() or \"caffeine\" in entity.text.lower():\n",
    "            product_entities.append(entity.text)\n",
    "    return product_entities\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next steps:\n",
    "- Explore the brand these reviews are for\n",
    "- Knowing the domain that this dataset is for, use transfer learning to build a relevant pre-trained model to improve."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using LSA Python Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of LSAModel\n",
    "lsa_model = LSAModel(df, tags=['NOUN'])\n",
    "lsa_model.get_topics()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of LDAGensim\n",
    "lda_model = LDAGensim(df, tags=['NOUN'])\n",
    "lda_model.get_topics()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
