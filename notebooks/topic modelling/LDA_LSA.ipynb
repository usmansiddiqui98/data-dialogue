{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47551261-d633-49dc-be52-1400e176ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jlow/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c73e5e-ed5d-4d53-b1d6-1e78477c0c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>review_length</th>\n",
       "      <th>clean_reviews</th>\n",
       "      <th>clean_review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>137</td>\n",
       "      <td>healthy dog food good digestion also good smal...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "      <td>350</td>\n",
       "      <td>pleased natural balance dog food dogs issues d...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "      <td>733</td>\n",
       "      <td>educated feline nutrition allowed cats become ...</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "      <td>493</td>\n",
       "      <td>holistic vet recommended along brands tried ca...</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "      <td>413</td>\n",
       "      <td>bought coffee much cheaper ganocafe organic re...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Sentiment        Time  \\\n",
       "0           0  positive  2021-06-18   \n",
       "1           1  positive  2021-07-07   \n",
       "2           2  positive  2021-06-18   \n",
       "3           3  positive  2021-07-07   \n",
       "4           4  positive  2021-01-07   \n",
       "\n",
       "                                                Text  review_length  \\\n",
       "0  This is a very healthy dog food. Good for thei...            137   \n",
       "1  I've been very pleased with the Natural Balanc...            350   \n",
       "2  Before I was educated about feline nutrition, ...            733   \n",
       "3  My holistic vet recommended this, along with a...            493   \n",
       "4  I bought this coffee because its much cheaper ...            413   \n",
       "\n",
       "                                       clean_reviews  clean_review_length  \n",
       "0  healthy dog food good digestion also good smal...                   94  \n",
       "1  pleased natural balance dog food dogs issues d...                  218  \n",
       "2  educated feline nutrition allowed cats become ...                  508  \n",
       "3  holistic vet recommended along brands tried ca...                  276  \n",
       "4  bought coffee much cheaper ganocafe organic re...                  218  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/processed/cleaned_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b991efd-f7d6-45ba-8d05-23c4d0b46e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34ec127-754a-4f7f-8451-df04d52b2327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5444, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(df['clean_reviews'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405e3e6e-7409-41df-9502-1ac071c9ca4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5456e543-1471-43c3-977b-042a385a326e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "[('taste', 0.25155210357167107), ('great', 0.24791012282601532), ('like', 0.24118936327602733), ('good', 0.22493736984778656), ('coffee', 0.20447658327986754), ('product', 0.18279133257619745), ('tea', 0.17242666623930542)]\n",
      "Topic 1: \n",
      "[('tea', 0.739290612844046), ('coffee', 0.3895882861231337), ('cup', 0.10851903125105143), ('green', 0.10713768208945683), ('drink', 0.08879795722257237), ('strong', 0.07391448433650427), ('flavor', 0.06996478550352822)]\n",
      "Topic 2: \n",
      "[('coffee', 0.7722157317953218), ('cup', 0.11821397635945038), ('roast', 0.0663484956367253), ('coffees', 0.060222826345661236), ('strong', 0.05658438835920417), ('beans', 0.05059874369740011), ('flavored', 0.04824775231210855)]\n",
      "Topic 3: \n",
      "[('price', 0.43120217572000663), ('product', 0.2384692284526262), ('amazon', 0.2188382239962651), ('great', 0.21409607159826988), ('tea', 0.1635448491385452), ('store', 0.13823498344412502), ('buy', 0.1265378028791918)]\n",
      "Topic 4: \n",
      "[('great', 0.7482200859014166), ('taste', 0.17206100513155925), ('love', 0.14338733418496571), ('tastes', 0.12359789470601627), ('snack', 0.08134337755570752), ('tasting', 0.0705708928488396), ('price', 0.05810479290907531)]\n",
      "Topic 5: \n",
      "[('product', 0.5489513427721556), ('water', 0.18682467472260705), ('drink', 0.1402130582563369), ('taste', 0.12590321370935023), ('tastes', 0.09589968841450103), ('good', 0.0916391193064251), ('like', 0.08726075423034917)]\n",
      "Topic 6: \n",
      "[('good', 0.5055498050588553), ('price', 0.3813291969220897), ('really', 0.1022115537003416), ('buy', 0.1001575333992431), ('chips', 0.09926836002468445), ('store', 0.0932885254756906), ('amazon', 0.09101534709659574)]\n",
      "Topic 7: \n",
      "[('chocolate', 0.5054675242393416), ('hot', 0.2764675383102803), ('love', 0.2075250928841431), ('best', 0.2075123418434068), ('milk', 0.15448805813343242), ('amazon', 0.12161836645897109), ('flavor', 0.12077565975160483)]\n",
      "Topic 8: \n",
      "[('product', 0.491765361732302), ('chips', 0.17610416691828892), ('tasty', 0.17109625991386332), ('chocolate', 0.1573775861659078), ('snack', 0.15492422693903696), ('good', 0.15213796647117017), ('cookies', 0.11776442419203409)]\n",
      "Topic 9: \n",
      "[('taste', 0.4851476113681041), ('like', 0.322050586650162), ('chips', 0.19987556020635855), ('love', 0.10830945073544061), ('buy', 0.09684337291814621), ('bag', 0.09521995895698819), ('salt', 0.0893678402748416)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    sentence = \"\"\n",
    "    print(sorted_terms)\n",
    "    # for t in sorted_terms:\n",
    "    #     print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f49891-ee9e-44b7-bd4e-59d6e68670b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LDA (1) using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9840c1b-888c-41c1-a93a-a7c0a3a14e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  0.38740405531745764 %\n",
      "Topic 0:\n",
      "taste, like, store, food, grocery, jars, one, good, almonds, would, best, licorice, box, amazon, local, little, really, baby, coconut, great\n",
      "Topic 1:\n",
      "fat, bag, calories, chips, per, serving, sodium, grams, protein, like, fiber, total, would, baked, also, great, little, nutrition, perfect, potato\n",
      "Topic 2:\n",
      "coffee, tea, like, taste, flavor, good, great, cup, one, love, price, tried, green, strong, drink, would, much, really, best, tastes\n",
      "Topic 3:\n",
      "product, money, waste, bought, time, could, buy, tried, worst, ever, better, stale, would, candies, return, one, many, away, great, purchased\n",
      "Topic 4:\n",
      "like, cookies, cookie, taste, spicy, one, milk, good, make, even, love, great, tastes, really, try, sauce, could, pods, want, ingredients\n",
      "Topic 5:\n",
      "hot, taste, chocolate, like, cocoa, one, would, cup, tried, product, keurig, cups, box, kcups, get, water, got, even, try, much\n",
      "Topic 6:\n",
      "would, salt, like, price, taste, buy, good, better, much, flavor, love, product, great, store, bought, way, chips, cookies, salty, tastes\n",
      "Topic 7:\n",
      "drink, taste, like, flavor, sugar, juice, water, sweet, good, orange, soda, would, fruit, flavors, great, really, drinks, calories, little, one\n",
      "Topic 8:\n",
      "like, taste, chocolate, good, great, love, sugar, flavor, butter, cereal, product, eat, one, sweet, peanut, healthy, milk, also, tastes, snack\n",
      "Topic 9:\n",
      "rice, soup, make, sauce, chicken, noodles, great, flavor, mix, good, taste, add, easy, really, like, also, flour, made, product, tasty\n",
      "Topic 10:\n",
      "bag, order, one, great, amazon, taste, time, love, product, would, like, good, find, get, eat, us, buy, go, even, much\n",
      "Topic 11:\n",
      "food, dog, one, treats, cat, dogs, made, china, cats, would, jerky, good, like, eat, chicken, product, get, since, old, time\n",
      "Topic 12:\n",
      "ingredients, food, dog, really, like, products, one, good, dogs, get, also, healthy, bought, foods, pet, tasty, sugar, going, made, package\n",
      "Topic 13:\n",
      "mix, free, cake, gluten, good, make, would, hot, like, better, product, chocolate, buy, water, flavor, baking, sauce, really, glutenfree, packet\n",
      "Topic 14:\n",
      "free, product, great, like, gluten, amazon, price, good, pasta, one, best, using, used, bread, organic, truffle, quality, found, also, get\n",
      "Topic 15:\n",
      "great, good, love, like, snack, product, taste, really, bars, healthy, loves, eat, easy, would, tastes, little, tasty, eating, recommend, cheese\n",
      "Topic 16:\n",
      "taste, good, best, great, water, price, would, coconut, like, one, really, time, protein, product, even, food, well, delicious, tried, organic\n",
      "Topic 17:\n",
      "price, product, one, store, seeds, great, would, amazon, cans, good, much, local, get, buy, order, got, found, stores, shipping, box\n",
      "Topic 18:\n",
      "product, amazon, shipping, item, received, ordered, great, arrived, service, ordering, would, happy, quickly, order, time, good, shipped, well, customer, price\n",
      "Topic 19:\n",
      "chips, flavor, popcorn, vanilla, beans, great, like, taste, cream, pork, kettle, oil, salt, good, used, get, bought, pepper, bag, love\n"
     ]
    }
   ],
   "source": [
    "# create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "# fit and transform the clean text data\n",
    "X = vectorizer.fit_transform(df['clean_reviews'])\n",
    "\n",
    "# Materialize the sparse data\n",
    "data_dense = X.todense()\n",
    "\n",
    "# Compute Sparsicity \n",
    "# Sparsicity is the percentage of non-zero datapoints in X\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "# create an LDA object and fit the data\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# print the top 20 words in each topic\n",
    "feature_names = sorted(vectorizer.vocabulary_.keys())\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    topic_complete = (\", \".join([feature_names[i] for i in topic.argsort()[:-21:-1]]))\n",
    "    print(topic_complete)\n",
    "    topic_list.append(topic_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a98b2c8-1125-4eb3-b096-8ac6d1fda147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -1290788.343878213\n",
      "Perplexity:  2141.8636295631304\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'batch',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 20,\n",
      " 'n_jobs': None,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 42,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda.score(X))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda.perplexity(X))\n",
    "\n",
    "# See model parameters\n",
    "pprint.pprint(lda.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16f344-7ebf-43e9-bc6e-79f8032ca757",
   "metadata": {},
   "source": [
    "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good.\n",
    "On a different note, perplexity might not be the best measure to evaluate topic models because it doesnâ€™t consider the context and semantic associations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfa7a6-9e30-4eb7-ba2e-ab1bd2bc2981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32d3f2-ce6f-440c-8dcc-c6d3a4f80208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b21d-959c-43d2-a7c0-d4738f3771d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a4bd9-981b-43a5-a69f-3cf0ff8e1ce2",
   "metadata": {},
   "source": [
    "It can be concluded that hyperparameter tuning has not been effective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac57663-9be8-4bef-bd58-5f0358415c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LDA (2) using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737eb90-fb42-4baa-bb67-cc2ca046e07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tokenized reviews without stop words\n",
    "tokenized_reviews = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for review in df['clean_reviews']:\n",
    "    tokens = word_tokenize(review)\n",
    "    tokens_without_stopwords = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    tokenized_reviews.append(tokens_without_stopwords)\n",
    "\n",
    "# create the id2word dictionary\n",
    "id2word = corpora.Dictionary(tokenized_reviews)\n",
    "\n",
    "# create the corpus\n",
    "corpus = [id2word.doc2bow(tokens) for tokens in tokenized_reviews]\n",
    "\n",
    "# create the LDA model\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=10)\n",
    "\n",
    "# visualize the topics using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84577e1a-cbfa-43f2-aa0d-6f2944cfcc32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8301e-5b78-4250-b28f-3fc6ff412839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03bdeb-b238-49be-b352-bf45508e87aa",
   "metadata": {},
   "source": [
    "#### Pet products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea38d98-3205-46e4-b9e4-00709c559eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pet_entities(text):\n",
    "    doc = nlp(text)\n",
    "    pet_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"ANIMAL\" or \"pet\" in entity.text.lower() or \"dog\" in entity.text.lower() or \"cat\" in entity.text.lower():\n",
    "            pet_entities.append(entity.text)\n",
    "    return pet_entities\n",
    "\n",
    "# apply the extract_pet_entities function to the reviews column\n",
    "df['pet_entities'] = df['clean_reviews'].apply(extract_pet_entities)\n",
    "\n",
    "# print the unique pet-related entities that were extracted\n",
    "pet_entities = set([entity for row in df['pet_entities'] for entity in row])\n",
    "print(pet_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149179d-7077-46a6-9556-3cbb8c68b700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(pet_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c693d3-e198-44b4-af8c-036b795dfc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['pet_entities'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71db254-441f-4bc0-bdfa-30aed79bac43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_product_entities(text):\n",
    "    doc = nlp(text)\n",
    "    product_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"PRODUCT\" or \"coffee\" in entity.text.lower() or \"tea\" in entity.text.lower() or \"caffeine\" in entity.text.lower():\n",
    "            product_entities.append(entity.text)\n",
    "    return product_entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf04fc-51c8-4445-b0de-ad8863db7aaa",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Explore the brand these reviews are for\n",
    "- Knowing the domain that this dataset is for, use transfer learning to build a relevant pre-trained model to improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
