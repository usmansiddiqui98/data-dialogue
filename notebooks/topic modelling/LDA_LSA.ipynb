{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47551261-d633-49dc-be52-1400e176ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c73e5e-ed5d-4d53-b1d6-1e78477c0c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/cleaned_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b991efd-f7d6-45ba-8d05-23c4d0b46e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ec127-754a-4f7f-8451-df04d52b2327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(df['clean_reviews'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e3e6e-7409-41df-9502-1ac071c9ca4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456e543-1471-43c3-977b-042a385a326e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    sentence = \"\"\n",
    "    print(sorted_terms)\n",
    "    # for t in sorted_terms:\n",
    "    #     print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f49891-ee9e-44b7-bd4e-59d6e68670b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LDA (1) using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9840c1b-888c-41c1-a93a-a7c0a3a14e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "# fit and transform the clean text data\n",
    "X = vectorizer.fit_transform(df['clean_reviews'])\n",
    "\n",
    "# Materialize the sparse data\n",
    "data_dense = X.todense()\n",
    "\n",
    "# Compute Sparsicity \n",
    "# Sparsicity is the percentage of non-zero datapoints in X\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "# create an LDA object and fit the data\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# print the top 20 words in each topic\n",
    "feature_names = sorted(vectorizer.vocabulary_.keys())\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    topic_complete = (\", \".join([feature_names[i] for i in topic.argsort()[:-21:-1]]))\n",
    "    print(topic_complete)\n",
    "    topic_list.append(topic_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98b2c8-1125-4eb3-b096-8ac6d1fda147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda.score(X))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda.perplexity(X))\n",
    "\n",
    "# See model parameters\n",
    "pprint.pprint(lda.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16f344-7ebf-43e9-bc6e-79f8032ca757",
   "metadata": {},
   "source": [
    "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good.\n",
    "On a different note, perplexity might not be the best measure to evaluate topic models because it doesnâ€™t consider the context and semantic associations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfa7a6-9e30-4eb7-ba2e-ab1bd2bc2981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32d3f2-ce6f-440c-8dcc-c6d3a4f80208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b21d-959c-43d2-a7c0-d4738f3771d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a4bd9-981b-43a5-a69f-3cf0ff8e1ce2",
   "metadata": {},
   "source": [
    "It can be concluded that hyperparameter tuning has not been effective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac57663-9be8-4bef-bd58-5f0358415c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LDA (2) using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737eb90-fb42-4baa-bb67-cc2ca046e07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tokenized reviews without stop words\n",
    "tokenized_reviews = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for review in df['clean_reviews']:\n",
    "    tokens = word_tokenize(review)\n",
    "    tokens_without_stopwords = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    tokenized_reviews.append(tokens_without_stopwords)\n",
    "\n",
    "# create the id2word dictionary\n",
    "id2word = corpora.Dictionary(tokenized_reviews)\n",
    "\n",
    "# create the corpus\n",
    "corpus = [id2word.doc2bow(tokens) for tokens in tokenized_reviews]\n",
    "\n",
    "# create the LDA model\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=10)\n",
    "\n",
    "# visualize the topics using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84577e1a-cbfa-43f2-aa0d-6f2944cfcc32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8301e-5b78-4250-b28f-3fc6ff412839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03bdeb-b238-49be-b352-bf45508e87aa",
   "metadata": {},
   "source": [
    "#### Pet products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea38d98-3205-46e4-b9e4-00709c559eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pet_entities(text):\n",
    "    doc = nlp(text)\n",
    "    pet_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"ANIMAL\" or \"pet\" in entity.text.lower() or \"dog\" in entity.text.lower() or \"cat\" in entity.text.lower():\n",
    "            pet_entities.append(entity.text)\n",
    "    return pet_entities\n",
    "\n",
    "# apply the extract_pet_entities function to the reviews column\n",
    "df['pet_entities'] = df['clean_reviews'].apply(extract_pet_entities)\n",
    "\n",
    "# print the unique pet-related entities that were extracted\n",
    "pet_entities = set([entity for row in df['pet_entities'] for entity in row])\n",
    "print(pet_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149179d-7077-46a6-9556-3cbb8c68b700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(pet_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c693d3-e198-44b4-af8c-036b795dfc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['pet_entities'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71db254-441f-4bc0-bdfa-30aed79bac43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_product_entities(text):\n",
    "    doc = nlp(text)\n",
    "    product_entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"PRODUCT\" or \"coffee\" in entity.text.lower() or \"tea\" in entity.text.lower() or \"caffeine\" in entity.text.lower():\n",
    "            product_entities.append(entity.text)\n",
    "    return product_entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf04fc-51c8-4445-b0de-ad8863db7aaa",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Explore the brand these reviews are for\n",
    "- Knowing the domain that this dataset is for, use transfer learning to build a relevant pre-trained model to improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
